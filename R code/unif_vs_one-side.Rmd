---
title: "Uniform outliers vs one-sided"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r, echo=FALSE}
#L1-hankelization
hankL1<- function(A) {
  N<-nrow(A)+ncol(A)-1
  F<-rep(0,N)
  L.1<-min(nrow(A),ncol(A))
  K.1<-max(nrow(A),ncol(A))
  for(k in 1:N) {
    v<-A[c(row(A) + col(A) - k == 1)]
    F[k]<-median(v)
  }
  return (F)
}

#L2-hankelization
hankL2<- function(A) {
  N<-nrow(A)+ncol(A)-1
  F<-rep(0,N)
  L.1<-min(nrow(A),ncol(A))
  K.1<-max(nrow(A),ncol(A))
  for(k in 1:N) {
    v<-A[c(row(A) + col(A) - k == 1)]
    F[k]<-mean(v)
  }
  return (F)
}

#L2-hankelization with weights 
#(is equal to L2-hankelization if W is hankel matrix)
hankL2_w<- function(A, W) {
  N<-nrow(A)+ncol(A)-1
  F<-rep(0,N)
  L.1<-min(nrow(A),ncol(A))
  K.1<-max(nrow(A),ncol(A))
  for(k in 1:N) {
    v<-A[c(row(A) + col(A) - k == 1)]
    w<-W[c(row(W) + col(W) - k == 1)]
    wv<-w*v
    zero.weights<- w!=rep(0,length(w))
    if (length(which(zero.weights == FALSE))==0){
      F[k]<-sum(wv)/sum(w)
    }
    else {F[k]<-mean(v)}
  }
  return (F)
}

library(QZ, quiet = TRUE)

weights<-function(R,sigma,a,m,n){
  x<-R/sigma
  W<-matrix(0L, nrow = m, ncol = n)
  W[which(abs(x)<=a,arr.ind=TRUE)]<-(1-(abs(x[which(abs(x)<=a,arr.ind=TRUE)])/a)^2)^2
  return(W)
}

CIRLS_mod<-function(M, k, trend.ver='loess', alpha=4.046, eps=1e-7, maxITER=10, maxiter=5){
  m<-nrow(M)
  n<-ncol(M)
  initial<-svd(M, nu=k, nv=k) #initialization U and V using svd
  U<-initial$u
  U <- as.matrix(U, nrow = nrow(U), ncol = k)
  Lambda<-initial$d
  V<-initial$v
  V <- as.matrix(V, nrow = nrow(), ncol = k)
  U<-U%*%diag(Lambda, nrow = k, ncol = k)
  ITER<-0
  iter<-0
  L<-m #window length for loess, lowess
  
  repeat {
    R<-M-U%*%H(V) #residuals matrix
    r<-as.vector(H(R))
    RR<-hankL1(R)
    RR.trmatrix<-hankel(RR,L=L)
    
    if (trend.ver == 'loess') { 
      loessMod30 <- loess(abs(RR) ~ c(1:length(abs(RR))), span=0.35)
      sigma <- predict(loessMod30)}
    
    else if (trend.ver == 'median') {
      sigma<-runmed(abs(RR),L/2+1)
      sigma[(length(RR)-L/4):length(RR)]<-calc.ends(abs(RR),L/4)}
    
    else if (trend.ver == 'lowess') {
      sigma<-lowess(c(1:length(RR)),abs(RR), f=0.35)$y}
    
    sigma.trmatrix<-hankel(sigma,L=L)
    W<-weights(RR.trmatrix,sigma.trmatrix,alpha,m,n) #weights matrix
    
    
    repeat{
      # updating U
      for (i in (1:m)){
        Wi<-diag(W[i,1:ncol(W)])
        mi<-M[i,1:ncol(M)]
        beta <- qr.solve(H(V)%*%Wi%*%V, H(V)%*%Wi%*%t(H(mi)))
        U[i,1:ncol(U)]<-H(beta)
      }
      # updating V
      for (j in (1:n)){
        Wj<-diag(W[1:nrow(W),j])
        mj<-M[1:nrow(M),j]
        beta <- qr.solve(H(U)%*%Wj%*%U, H(U)%*%Wj%*%mj)
        V[j,1:ncol(V)]<-H(beta)
      }
      iter<-iter+1
      temp <- W^{1/2}*(M-U%*%H(V))
      if ( ((abs(sum(diag(temp%*%H(temp)))))<eps) | (iter > maxiter) ) {break}
    }
    
    ITER<-ITER+1
    temp <- W^{1/2}*(M-U%*%H(V))
    if ( ((abs(sum(diag(temp%*%H(temp)))))<eps) | (ITER > maxITER) ) { break}
  }
  M_est<-U%*%H(V)
  return(M_est)
}

#Original IRLS method
# M --- trajectory matrix
# k --- rank of signal
# maxITER --- number of iterations N_IRLS
# maxiter --- number of iterations N_alpha

library(QZ, quiet = TRUE)

weights<-function(R,sigma,a,m,n){
  x<-R/sigma
  W<-matrix(0L, nrow = m, ncol = n)
  W[which(abs(x)<=a,arr.ind=TRUE)]<-(1-(abs(x[which(abs(x)<=a,arr.ind=TRUE)])/a)^2)^2
  return(W)
}

IRLS_complex<-function(M, k, alpha=4.685, eps=1e-5, maxITER=10, maxiter=5){
  m<-nrow(M)
  n<-ncol(M)
  initial<-svd(M, nu=k, nv=k) #initialization U and V using svd
  U<-initial$u
  U <- as.matrix(U, nrow = nrow(U), ncol = k)
  Lambda<-initial$d
  V<-initial$v
  U<-U%*%diag(Lambda, nrow = k, ncol = k)
  ITER<-0
  iter<-0
  V <- as.matrix(V, nrow = nrow(), ncol = k)
  repeat {
    R<-M-U%*%H(V) #residuals matrix
    r<-as.vector(H(R))
    sigma<-1.4826*median(abs(r-median(abs(r))))
    W<-weights(R,sigma,alpha,m,n)
    #W <- matrix(1, m, n)
    
    repeat{
      # updating U
      for (i in (1:m)){
        Wi<-diag(W[i,1:ncol(W)])
        mi<-M[i,1:ncol(M)]
        beta <- qr.solve(H(V)%*%Wi%*%V, H(V)%*%Wi%*%t(H(mi)))
        U[i,1:ncol(U)]<-H(beta)
      }
      # updating V
      for (j in (1:n)){
        Wj<-diag(W[1:nrow(W),j])
        mj<-M[1:nrow(M),j]
        beta <- qr.solve(H(U)%*%Wj%*%U, H(U)%*%Wj%*%mj)
        V[j,1:ncol(V)]<-H(beta)
      }
      iter<-iter+1
      temp <- W^{1/2}*(M-U%*%H(V))
      if ( ((abs(sum(diag(temp%*%H(temp)))))<eps) | (iter > maxiter) ) {break}
    }
    
    ITER<-ITER+1
    temp <- W^{1/2}*(M-U%*%H(V))
    if ( ((abs(sum(diag(temp%*%H(temp)))))<eps) | (ITER > maxITER) ) { break}
  }
  M_est<-U%*%H(V)
  return(M_est)
}

library(BBmisc)
library(spatstat)

l1_complex<-function(M, k, eps=1e-5, maxiter=10){
  m<-nrow(M)
  n<-ncol(M)
  initial<-svd(M, nu=k, nv=k) #initialization U and V using svd
  U<-initial$u
  U <- as.matrix(U, nrow = nrow(U), ncol = k)
  V<-initial$v
  V <- as.matrix(V, nrow = nrow(), ncol = k)
  V <- normalize(V, method = "scale", margin = 2)
  ITER<-0
  iter<-0
    
  repeat{
    V_old <- V
    
    # updating U
    for (i in (1:m)){
      mi<-M[i,1:ncol(M)]
      for (c in 1:k) {
        vc <- V[1:nrow(V), c]
        beta <- weighted.median(Re(t(H(mi))/vc), abs(vc)) + complex(imaginary = weighted.median(Im(t(H(mi))/vc), abs(vc)))
        U[i, c]<-H(beta) 
      }
    }
    
    # updating V
    for (j in (1:n)){
      mj<-M[1:nrow(M),j]
      for (c in 1:k) {
        uc <- U[1:nrow(U), c]
        beta <- weighted.median(Re(mj/uc), abs(uc)) +  complex(imaginary = weighted.median(Im(mj/uc), abs(uc)))
        V[j, c]<-H(beta) 
      }
    }
    V <- normalize(V, method = "scale", margin = 2)
  
    iter<-iter+1
    if  ((max(abs(Re(V) - Re(V_old)) + abs(Im(V) - Im(V_old)))<eps) | (iter > maxiter) ) {break}
  }
  
  M_est<-U%*%H(V)
  return(M_est)
}

library(Rssa)
library(matrixcalc)
library(cmvnorm)
library(pcaL1)
```

Функция для вычисления p-value

```{r}
p_val_rmse <- function(x, y, n = 30) {
  sd_x <- sd(x)
  sd_y <- sd(y)
  t <- sqrt(n) * (mean(x) - mean(y)) / sqrt(sd_x^2 + sd_y^2 - 2 * sd_x * sd_y * cor(x, y))
  if (t > 0) {
    return(2 - 2 *pnorm(t))
  }
  return(2*pnorm(t))
}
```

Требуется сравнить, как ведут себя методы для случая "равномерных" выбросов и случая, когда выбросы сосредоточены в вещественной или мнимой области. НУО выберем вещественную.
Чтобы сравнение было осмысленным, было предложено две возможных связи для первого и второго случая: сохранение модулей выбросов и сохранение квадратов модулей выбросов.
Имеем вы $x_n = e^{4n/N} e^{2n\pi/30i} + \frac{1}{2}e^{4n/N} \varepsilon_n$ со значением выброса $5 x_n$.
Таким образом в одной задаче требуется разрешить ур-ие $\sqrt{25x^2 + 25y^2} = \sqrt{(x + z)^2 + y^2}$ относительно $z$, а в другой $25x^2 + 25y^2 = (x + z)^2 + y^2$.
Как мы видим, уравнения эквивалентны. Здесь $z$ - то значение, которое мы хотим добавить к вещественной части, для получения нами интересующего выброса, а $x_n = 5(x + iy)$, если это выброс.
При решении уравнения встаёт вопрос о двойственности и знака $z$. Для ественности модели будет логичным принять $z$ того же знака, что и $x$. Тем более, что мы всё равно получим удовлетворительный ответ.
Тогда $z = sign(x) \sqrt{25 x^2 + 24 y^2} - x$.
Построим данный новый ряд и посчитаем RMSE, RMSE для вещественной части и RMSE для мнимой части на 30 испытаниях. Здесь оба случая рассматриваются на одной и той же выборке, поскольку случайные элементы outlier.seq и noise не изменяются с момента введнеия и до окончания итерации цикла и используются для обоих рядов.

```{r, warning=FALSE}
f1 <- vector()
f2 <- vector()
f3 <- vector()
f4 <- vector()

ref1 <- vector()
ref2 <- vector()
ref3 <- vector()
ref4 <- vector()

imf1 <- vector()
imf2 <- vector()
imf3 <- vector()
imf4 <- vector()

g1 <- vector()
g2 <- vector()
g3 <- vector()
g4 <- vector()

reg1 <- vector()
reg2 <- vector()
reg3 <- vector()
reg4 <- vector()

img1 <- vector()
img2 <- vector()
img3 <- vector()
img4 <- vector()


N<-240
set.seed(1)

sig <- exp(4*(1:N)/N)*exp(2i*pi*(1:N)/30)
rnk<-1

for (i in 1:30) {
  sig.outl<-sig
  sig.outl1<-sig
  
  outlier.seq<-sample(1:(N),N*0.05)
  
  sig.outl[outlier.seq]<-sig.outl[outlier.seq] + 4 * sig.outl[outlier.seq]
  sig.outl1[outlier.seq] <- sig.outl1[outlier.seq] + sign(Re(sig.outl1[outlier.seq]))*sqrt(25 * Re(sig.outl1[outlier.seq])^2 + 24 * Im(sig.outl1[outlier.seq])^2) - Re(sig.outl1[outlier.seq])
  
  noise <- 0.5*exp(4*(1:N)/N)*rcnorm(N)
  
  ser<-sig.outl + noise
  ser1 <- sig.outl1 + noise
    

  X<-hankel(ser, L=120)

  Pr<-IRLS_complex(X, rnk) #weighted L2
  Pr0<-hankL2(Pr)

  Pr<-l1_complex(X, rnk) #l1pca
  Pr.L1<-hankL1(Pr)

  Pr<-CIRLS_mod(X,rnk,'loess') #CIRLS modification (trend extraction with loess)
  Pr1<-hankL2(Pr)

  s <- ssa(ser, kind = "cssa")
  r <- reconstruct(s, groups = list(Trend = 1))

  
  X<-hankel(ser1, L=120)
  
  Pr<-IRLS_complex(X, rnk) #weighted L2
  Pr01<-hankL2(Pr)
  
  Pr<-l1_complex(X, rnk) #l1pca
  Pr.L11<-hankL1(Pr)
  
  Pr<-CIRLS_mod(X,rnk,'loess') #CIRLS modification (trend extraction with loess)
  Pr11<-hankL2(Pr)
  
  s1 <- ssa(ser1, kind = "cssa")
  r1 <- reconstruct(s1, groups = list(Trend = 1))


  f1 <- c(f1, sqrt(mean((Re(sig) - Re(r$Trend))^2) + mean((Im(sig) - Im(r$Trend))^2)))
  f2 <- c(f2, sqrt(mean((Re(sig) - Re(Pr.L1))^2) + mean((Im(sig) - Im(Pr.L1))^2)))
  f3 <- c(f3, sqrt(mean((Re(sig) - Re(Pr0))^2) + mean((Im(sig) - Im(Pr0))^2)))
  f4 <- c(f4, sqrt(mean((Re(sig) - Re(Pr1))^2) + mean((Im(sig) - Im(Pr1))^2)))
  
  ref1 <- c(ref1, sqrt(mean((Re(sig) - Re(r$Trend))^2)))
  ref2 <- c(ref2, sqrt(mean((Re(sig) - Re(Pr.L1))^2)))
  ref3 <- c(ref3, sqrt(mean((Re(sig) - Re(Pr0))^2)))
  ref4 <- c(ref4, sqrt(mean((Re(sig) - Re(Pr1))^2)))
  
  imf1 <- c(imf1, sqrt(mean((Im(sig) - Im(r$Trend))^2)))
  imf2 <- c(imf2, sqrt(mean((Im(sig) - Im(Pr.L1))^2)))
  imf3 <- c(imf3, sqrt(mean((Im(sig) - Im(Pr0))^2)))
  imf4 <- c(imf4, sqrt(mean((Im(sig) - Im(Pr1))^2)))
  
  g1 <- c(g1, sqrt(mean((Re(sig) - Re(r1$Trend))^2) + mean((Im(sig) - Im(r1$Trend))^2)))
  g2 <- c(g2, sqrt(mean((Re(sig) - Re(Pr.L11))^2) + mean((Im(sig) - Im(Pr.L11))^2)))
  g3 <- c(g3, sqrt(mean((Re(sig) - Re(Pr01))^2) + mean((Im(sig) - Im(Pr01))^2)))
  g4 <- c(g4, sqrt(mean((Re(sig) - Re(Pr11))^2) + mean((Im(sig) - Im(Pr11))^2)))
  
  reg1 <- c(reg1, sqrt(mean((Re(sig) - Re(r1$Trend))^2)))
  reg2 <- c(reg2, sqrt(mean((Re(sig) - Re(Pr.L11))^2)))
  reg3 <- c(reg3, sqrt(mean((Re(sig) - Re(Pr01))^2)))
  reg4 <- c(reg4, sqrt(mean((Re(sig) - Re(Pr11))^2)))
  
  img1 <- c(img1, sqrt(mean((Im(sig) - Im(r1$Trend))^2)))
  img2 <- c(img2, sqrt(mean((Im(sig) - Im(Pr.L11))^2)))
  img3 <- c(img3, sqrt(mean((Im(sig) - Im(Pr01))^2)))
  img4 <- c(img4, sqrt(mean((Im(sig) - Im(Pr11))^2)))
}
```

Посчтиаем p-value для гипотезы об отличимости RMSE результата работы одного метода на двух рядах для каждого из методов.

```{r, warning=FALSE}
print("L1, weighted L2, modified weighted L2 (trend extraction with loess)")
print("p-value:")
print(c(p_val_rmse(f2, g2), p_val_rmse(f3, g3), p_val_rmse(f4, g4)))
print("p-value for real part:")
print(c(p_val_rmse(ref2, reg2), p_val_rmse(ref3, reg3), p_val_rmse(ref4, reg4)))
print("p-value for imagine part:")
print(c(p_val_rmse(imf2, img2), p_val_rmse(imf3, img3), p_val_rmse(imf4, img4)))
```
Заметим, что p-value < 0.05 только для метода L1, посмотрим с каким рядом данный метод справляется лучше.

```{r}
print("RMSE for uniform outliers")
print(mean(f2))
print("RMSE for one-sided outliers")
print(mean(g2))
```

Таким образом, при уровне значимости 0.05, можно предположить, что метод L1 показывает лучший результат для ряда с односторонними выбросами, остальные же методы показывают неотличимые результаты.


